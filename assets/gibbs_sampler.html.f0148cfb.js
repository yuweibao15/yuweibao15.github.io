import{_ as s}from"./_plugin-vue_export-helper.cdc0426e.js";import{o as n,c as a,b as e,d as r,f as i,e as o,r as l}from"./app.31621451.js";const c={},f=i('<p>Gibbs sampling is a special case of MH algorithms. Gibbs sampling involves sampling from the conditional distributions of the variables of interest given the values of the other variables until the chain converges to a stationary distribution. Gibbs sampling is very useful for inference under linear models (the prior, the likelihood, and the posterior are all normal distributions) <sup class="footnote-ref"><a href="#footnote1">[1]</a><a class="footnote-anchor" id="footnote-ref1"></a></sup> and for sampling from high-dimensional distributions. However, it requires the knowledge of conditional distributions.</p><p>Let&#39;s understand Gibbs sampling by some R codes <sup class="footnote-ref"><a href="#footnote2">[2]</a><a class="footnote-anchor" id="footnote-ref2"></a></sup></p><hr class="footnotes-sep">',3),d={class:"footnotes"},h={class:"footnotes-list"},_=e("li",{id:"footnote1",class:"footnote-item"},[e("p",null,[o("Z. Yang. Molecular Evolution: A Statistical Approach. Oxford Univ. Press, 2014. "),e("a",{href:"#footnote-ref1",class:"footnote-backref"},"\u21A9\uFE0E")])],-1),p={id:"footnote2",class:"footnote-item"},u={href:"http://www2.stat.duke.edu/~rcs46/modern_bayes17/lecturesModernBayes17/lecture-7/07-gibbs.pdf",target:"_blank",rel:"noopener noreferrer"},b=o("http://www2.stat.duke.edu/~rcs46/modern_bayes17/lecturesModernBayes17/lecture-7/07-gibbs.pdf"),m=o(),g=e("a",{href:"#footnote-ref2",class:"footnote-backref"},"\u21A9\uFE0E",-1);function v(k,w){const t=l("ExternalLinkIcon");return n(),a("div",null,[f,e("section",d,[e("ol",h,[_,e("li",p,[e("p",null,[e("a",u,[b,r(t)]),m,g])])])])])}const B=s(c,[["render",v],["__file","gibbs_sampler.html.vue"]]);export{B as default};
